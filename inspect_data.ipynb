{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiavwvlvHvM1"
      },
      "source": [
        "# Perception Test Dataset Visualization\n",
        "\n",
        "This Colab demonstrates how to access, parse and visualise the Perception Test dataset freely available at https://storage.cloud.google.com/dm-perception-test\n",
        "\n",
        "Perception Test dataset is a multimodal dataset that aims to comprehensively\n",
        "evaluate perception and reasoning skills of multimodal models. The dataset introduces real-world videos designed to show perceptually interesting situations and defines multiple tasks that require understanding of memory, abstract patterns, physics, and semantics â€“ across visual, audio, and text modalities.\n",
        "\n",
        "The dataset, composed of training and validation splits, consists of 11.7k videos (with audio), of 23s average length, and filmed by\n",
        "around 100 participants worldwide. The videos are annotated with six types of labels: object and point\n",
        "tracks, temporal action and sound segments, multiple-choice video question-answers and grounded\n",
        "video question-answers. The dataset probes pre-trained models for their transfer capabilities, in\n",
        "either zero-shot or finetuning regime.\n",
        "\n",
        "###Quick view\n",
        "1. Click \"Connect\" in the top right corner.\n",
        "1. Select __\"Runtime -\u003e Run all\"__.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnSmLrJH_BJv"
      },
      "source": [
        "# Initialise and Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P51hwXO5INxk"
      },
      "outputs": [],
      "source": [
        "#@title Prerequisites\n",
        "\n",
        "!pip install chex\n",
        "!pip3 install imageio==2.4.1\n",
        "!pip3 install ml_collections\n",
        "!pip install -q mediapy\n",
        "\n",
        "import abc\n",
        "from absl import logging\n",
        "import chex\n",
        "import colorsys\n",
        "import copy\n",
        "import cv2\n",
        "import imageio\n",
        "import io\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy\n",
        "import moviepy.editor as mvp\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import PIL\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ml_collections import config_dict\n",
        "from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple, Type, TypeVar, Union\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DtVxGBSNMbl4"
      },
      "outputs": [],
      "source": [
        "#@title Load the TFRecord data from GCP\n",
        "\n",
        "dataset = \"base_oss\"\n",
        "split = \"test\"  # @param [\"train\", \"test\"]\n",
        "\n",
        "def load_a_sequence_example(dataset, split):\n",
        "  print(f\"Loading dataset {dataset}; split {split}\")\n",
        "  tfrecord_uri = f\"gs://dm-perception-test/tfrecords/demo/{dataset}/{split}/perception_test__{split}-*-of-*.tfrecord\"\n",
        "\n",
        "  filenames = tf.io.matching_files(tfrecord_uri, name=None)\n",
        "  filenames = [tf.compat.as_str_any(tensor.numpy()) for tensor in filenames]\n",
        "  filenames.sort()\n",
        "  print(f\"Files {filenames}\")\n",
        "  \n",
        "  ds = tf.data.TFRecordDataset(filenames)\n",
        "  # Pick first two and shuffle.\n",
        "  ds = ds.shuffle(2)\n",
        "  ds_iter = ds.as_numpy_iterator()\n",
        "  serialised_example = ds_iter.next()\n",
        "  return tf.train.SequenceExample.FromString(serialised_example)\n",
        "\n",
        "sequence_example = load_a_sequence_example(dataset, split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Bd6d_4Bd_yTa"
      },
      "outputs": [],
      "source": [
        "#@title Peek into SequenceExample\n",
        "show_raw_example = False  #@param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "FT = TypeVar(\"FT\")\n",
        "_FEATURE_ACCESS = {\n",
        "    bytes: lambda x: x.bytes_list.value,\n",
        "    str: lambda x: [s.decode(\"utf-8\") for s in x.bytes_list.value],\n",
        "    int: lambda x: x.int64_list.value,\n",
        "    float: lambda x: x.float_list.value,\n",
        "}\n",
        "\n",
        "\n",
        "def get_features(\n",
        "    example: tf.train.SequenceExample, dtype: Type[FT], feature: str\n",
        ") -\u003e List[List[FT]]:\n",
        "  read_feature = _FEATURE_ACCESS[dtype]\n",
        "  feature_list = example.feature_lists.feature_list.get(feature, None)\n",
        "  if feature_list is None:\n",
        "    return []\n",
        "  return [read_feature(feature) for feature in feature_list.feature]\n",
        "\n",
        "\n",
        "if show_raw_example:\n",
        "  def list_features(feature, f_type):\n",
        "    print(f\"\\n{feature}\")\n",
        "    data = get_features(sequence_example, f_type, feature)\n",
        "    for d in data:\n",
        "      print(data)\n",
        "\n",
        "  print(\"SequenceExample Features\")\n",
        "  for key in sequence_example.feature_lists.feature_list.keys():\n",
        "    print(key)\n",
        "\n",
        "  ids = get_features(sequence_example, int, \"objects/track_id\")\n",
        "  labels = get_features(sequence_example, str, \"objects/label\")\n",
        "  print(f\"\\nNumber of tracked objects: {len(ids)}\")\n",
        "  for i, (id, label) in enumerate(zip(ids, labels)):\n",
        "    print(f\"id: {id[0]:02} - {label[0]:14} (track {i:02})\")\n",
        "\n",
        "  ids = get_features(sequence_example, int, \"points/track_id\")\n",
        "  labels = get_features(sequence_example, str, \"points/label\")\n",
        "  print(f\"\\nNumber of tracked points: {len(ids)}\")\n",
        "  for i, (id, label) in enumerate(zip(ids, labels)):\n",
        "    print(f\"id: {id[0]:02} - {label[0]:14} (track {i:02})\")\n",
        "\n",
        "  ids = get_features(sequence_example, int, \"questions/type\")\n",
        "  print(f\"\\nNumber of tracked points: {len(ids)}\")\n",
        "  for i, (typeid, label) in enumerate(zip(ids, labels)):\n",
        "    print(f\"id: {id[0]:02} - {label[0]:14} (track {i:02})\")\n",
        "\n",
        "  FEATURE_LIST = [\n",
        "      (\"questions/type\", str),\n",
        "      (\"questions/multi_answer/answer_ids\", int),\n",
        "      (\"questions/subcategory\", str),\n",
        "      (\"questions/domain\", str),\n",
        "      (\"questions/reasoning\", str),\n",
        "      (\"questions/task_id\", str),\n",
        "      (\"questions/multi_choice/answer_id\", int),\n",
        "      (\"questions/tags\", str),\n",
        "      (\"questions\", str),\n",
        "      (\"actions/track_id\", int),\n",
        "  ]\n",
        "\n",
        "  for feature, data_type in FEATURE_LIST:\n",
        "    list_features(feature, data_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAI7TFqDAZ4B"
      },
      "source": [
        "# Parsing SequenceExample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "82oHLxEftvVq"
      },
      "outputs": [],
      "source": [
        "#@title ParseExample Classes\n",
        "\n",
        "@chex.dataclass\n",
        "class ExampleMetadata:\n",
        "  \"\"\"Global data about this Perception Test example.\"\"\"\n",
        "  original_audio_sample_rate: float\n",
        "  original_audio_start_time: float\n",
        "  original_audio_num_samples: int\n",
        "\n",
        "  original_video_frame_rate: float\n",
        "  original_video_frames: int\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -\u003e \"ExampleMetadata\":\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    audio_dims = example.context.feature[\"WAVEFORM/feature/dimensions\"]\n",
        "    [num_audio_samples] = audio_dims.int64_list.value\n",
        "    sample_rate_feat = example.context.feature[\"WAVEFORM/feature/sample_rate\"]\n",
        "    [audio_sample_rate] = sample_rate_feat.float_list.value\n",
        "    [audio_start_us] = get_features(\n",
        "        example, int, \"WAVEFORM/feature/timestamp\")\n",
        "    audio_start_us = audio_start_us[0]\n",
        "    num_video_frames = len(get_features(example, int, \"image/timestamp\"))\n",
        "    frame_rate_feat = example.context.feature[\"image/frame_rate\"]\n",
        "    [video_frame_rate] = frame_rate_feat.float_list.value\n",
        "    return cls(\n",
        "        original_audio_sample_rate=audio_sample_rate,\n",
        "        original_audio_start_time=audio_start_us / 1e6,\n",
        "        original_audio_num_samples=num_audio_samples,\n",
        "        original_video_frame_rate=video_frame_rate,\n",
        "        original_video_frames=num_video_frames)\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class PointTrack:\n",
        "  \"\"\"Single point tracked across a video.\"\"\"\n",
        "\n",
        "  points: chex.Array           # [frames, 2] -- y, x\n",
        "  frames: chex.Array           # [frames]\n",
        "  human_annotated: chex.Array  # [frames]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -\u003e List[\"PointTrack\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    ys = get_features(example, float, \"points/points/y\")\n",
        "    xs = get_features(example, float, \"points/points/x\")\n",
        "    is_human = get_features(example, int, \"points/points/is_human_label\")\n",
        "    frames = get_features(example, int, \"points/points/frame\")\n",
        "    num_points = len(ys)\n",
        "    if not num_points:\n",
        "      return []\n",
        "    for f in [ys, xs, is_human, frames]:\n",
        "      if len(f) != num_points:\n",
        "        raise ValueError(\"Invalid number of point features.\")\n",
        "    return [  # pylint: disable=g-complex-comprehension\n",
        "        cls(points=np.stack([ys[b], xs[b]],\n",
        "                            axis=-1).clip(0, 1).astype(np.float32),\n",
        "            frames=np.asarray(frames[b], dtype=np.int32),\n",
        "            human_annotated=np.asarray(is_human[b], dtype=bool))\n",
        "        for b in range(num_points)\n",
        "    ]\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class BoxTrack:\n",
        "  \"\"\"Single object bounding boxes across a video.\"\"\"\n",
        "\n",
        "  track_id: int\n",
        "  label: str\n",
        "  boxes: chex.Array            # [frames, 4] -- y1, x1, y2, x2\n",
        "  frames: chex.Array           # [frames]\n",
        "  human_annotated: chex.Array  # [frames]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -\u003e List[\"BoxTrack\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    track_ids = get_features(example, int, \"objects/track_id\")\n",
        "    track_labels = get_features(example, str, \"objects/label\")\n",
        "    y1s = get_features(example, float, \"objects/bounding_boxes/top_left_y\")\n",
        "    x1s = get_features(example, float, \"objects/bounding_boxes/top_left_x\")\n",
        "    y2s = get_features(example, float, \"objects/bounding_boxes/bottom_right_y\")\n",
        "    x2s = get_features(example, float, \"objects/bounding_boxes/bottom_right_x\")\n",
        "    is_human = get_features(\n",
        "        example, int, \"objects/bounding_boxes/is_human_label\")\n",
        "    frames = get_features(example, int, \"objects/bounding_boxes/frame\")\n",
        "    num_boxes = len(y1s)\n",
        "    if not num_boxes:\n",
        "      return []\n",
        "    for f in [y1s, x1s, y2s, x2s, is_human, frames, track_ids, track_labels]:\n",
        "      if len(f) != num_boxes:\n",
        "        raise ValueError(\"Invalid number of box features.\")\n",
        "    return [  # pylint: disable=g-complex-comprehension\n",
        "        cls(track_id=track_ids[b][0],\n",
        "            label=track_labels[b][0],\n",
        "            boxes=np.stack([y1s[b], x1s[b],\n",
        "                            y2s[b], x2s[b]], axis=-1\n",
        "                           ).clip(0, 1).astype(np.float32),\n",
        "            frames=np.asarray(frames[b], dtype=np.int32),\n",
        "            human_annotated=np.asarray(is_human[b], dtype=bool))\n",
        "        for b in range(num_boxes)\n",
        "    ]\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class AudioBox:\n",
        "  \"\"\"The start and end position of a sound within a video.\"\"\"\n",
        "\n",
        "  start_time: chex.Numeric  # Time since the start of the video in seconds.\n",
        "  end_time: chex.Numeric\n",
        "  audio_label: str  # E.g. \"Human:Speech\"\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -\u003e List[\"AudioBox\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    starts = get_features(example, int, \"sounds/start_timestamp\")\n",
        "    ends = get_features(example, int, \"sounds/end_timestamp\")\n",
        "    labels = get_features(example, str, \"sounds/label\")\n",
        "    num_sounds = len(starts)\n",
        "    if not num_sounds:\n",
        "      return []\n",
        "    for f in [starts, ends, labels]:\n",
        "      if len(f) != num_sounds:\n",
        "        raise ValueError(\"Invalid number of audio features.\")\n",
        "    return [  # pylint: disable=g-complex-comprehension\n",
        "        cls(start_time=start[0] / 1e6, end_time=end[0] / 1e6,\n",
        "            audio_label=label[0].strip())\n",
        "        for start, end, label in zip(starts, ends, labels)\n",
        "    ]\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class MultipleChoiceQuestion:\n",
        "  \"\"\"Multiple choice questions.\"\"\"\n",
        "\n",
        "  text: str\n",
        "  options: List[str]\n",
        "  answer_id: int\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls,\n",
        "            example: tf.train.SequenceExample\n",
        "            ) -\u003e List[\"MultipleChoiceQuestion\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    questions = get_features(example, str, \"questions/question\")\n",
        "    qtypes = get_features(example, str, \"questions/type\")\n",
        "    options = get_features(example, str, \"questions/multi_choice/options\")\n",
        "    answer_ids = get_features(example, int, \"questions/multi_choice/answer_id\")\n",
        "    if not questions or not options or not answer_ids:\n",
        "      return []\n",
        "    if not len(questions) == len(options) == len(answer_ids) == len(qtypes):\n",
        "      raise ValueError(\n",
        "          f\"Invalid example with #q={len(questions)}, \"\n",
        "          f\"#o={len(options)}, #a={len(answer_ids)}, #t={len(qtypes)}.\")\n",
        "    ret = []\n",
        "    for i in range(len(questions)):\n",
        "      [qtype] = qtypes[i]\n",
        "      if qtype != \"LANGUAGE\":\n",
        "        continue\n",
        "      [question] = questions[i]\n",
        "      if not options[i]:\n",
        "        continue\n",
        "      [answer_id] = answer_ids[i]\n",
        "      ret.append(cls(text=question, options=options[i], answer_id=answer_id))\n",
        "    return ret\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class GroundedObjectQuestion:\n",
        "  \"\"\"Grounded object detection questions.\"\"\"\n",
        "\n",
        "  text: str\n",
        "  box_track_ids: List[int]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls,\n",
        "            example: tf.train.SequenceExample,\n",
        "            available_box_tracks: Set[int],\n",
        "            ) -\u003e List[\"GroundedObjectQuestion\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    questions = get_features(example, str, \"questions/question\")\n",
        "    qtypes = get_features(example, str, \"questions/type\")\n",
        "    answer_ids = get_features(example, int, \"questions/multi_answer/answer_ids\")\n",
        "    if not questions or not answer_ids:\n",
        "      return []\n",
        "    if not len(questions) == len(answer_ids) == len(qtypes):\n",
        "      raise ValueError(\n",
        "          f\"Invalid example with #q={len(questions)}, \"\n",
        "          f\"#a={len(answer_ids)}, #t={len(qtypes)}.\")\n",
        "    ret = []\n",
        "    for i in range(len(questions)):\n",
        "      [qtype] = qtypes[i]\n",
        "      if qtype != \"BOX\":\n",
        "        continue\n",
        "      [question] = questions[i]\n",
        "      if not answer_ids[i]:\n",
        "        continue\n",
        "      box_track_ids = [track_id for track_id in answer_ids[i]\n",
        "                       if track_id in available_box_tracks]\n",
        "      if not box_track_ids:\n",
        "        continue\n",
        "      ret.append(cls(text=question, box_track_ids=box_track_ids))\n",
        "    return ret\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class VideoAction:\n",
        "  \"\"\"The data class for the storing actions in video.\"\"\"\n",
        "  track_id: int\n",
        "  start_frame: int\n",
        "  end_frame: int\n",
        "  start_time: int\n",
        "  end_time: int\n",
        "  label: str\n",
        "\n",
        "  @classmethod\n",
        "  def parse(\n",
        "      cls,\n",
        "      example: tf.train.SequenceExample,\n",
        "  ) -\u003e List[\"VideoAction\"]:\n",
        "    \"\"\"Parses input data for the temporal action localization task.\n",
        "\n",
        "    Args:\n",
        "      example: Initial tf.train.SequenceExample from which data is extracted.\n",
        "\n",
        "    Returns:\n",
        "      List of VideoAction objects. One per sample.\n",
        "    \"\"\"\n",
        "    track_ids = get_features(example, int, \"actions/track_id\")\n",
        "    start_frames = get_features(example, int, \"actions/start_frame\")\n",
        "    start_timestamps = get_features(example, int, \"actions/start_timestamp\")\n",
        "    end_frames = get_features(example, int, \"actions/end_frame\")\n",
        "    end_timestamps = get_features(example, int, \"actions/end_timestamp\")\n",
        "    labels = get_features(example, str, \"actions/label\")\n",
        "    result = []\n",
        "    data_iterator = zip(\n",
        "        track_ids,\n",
        "        start_frames,\n",
        "        end_frames,\n",
        "        start_timestamps,\n",
        "        end_timestamps,\n",
        "        labels,\n",
        "    )\n",
        "    for sample in data_iterator:\n",
        "      track_id, start_frame, end_frame, start_time, end_time, label = sample\n",
        "      result.append(\n",
        "          cls(\n",
        "              track_id=track_id,\n",
        "              start_frame=start_frame,\n",
        "              end_frame=end_frame,\n",
        "              start_time=start_time,\n",
        "              end_time=end_time,\n",
        "              label=label\n",
        "          ))\n",
        "    return result\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class ParsedExample:\n",
        "  \"\"\"Parsed Perception Test example.\"\"\"\n",
        "  metadata: ExampleMetadata\n",
        "  video_frames: Optional[chex.Array]  # [num_frames, h, w, c]\n",
        "  video_features: Optional[chex.Array]\n",
        "  video_actions: Optional[Sequence[VideoAction]]\n",
        "  audio_wav: Optional[chex.Array]     # [num_samples]\n",
        "  point_tracks: List[PointTrack]\n",
        "  box_tracks: List[BoxTrack]\n",
        "  audio_boxes: List[AudioBox]\n",
        "  grounded_object_questions: List[GroundedObjectQuestion]\n",
        "  multiple_choice_questions: List[MultipleChoiceQuestion]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -\u003e \"ParsedExample\":\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    metadata = ExampleMetadata.parse(example)\n",
        "    audio_wav = load_audio(example)\n",
        "    video_frames = load_video(example)\n",
        "    point_tracks = PointTrack.parse(example)\n",
        "    box_tracks = BoxTrack.parse(example)\n",
        "    audio_boxes = AudioBox.parse(example)\n",
        "    # Sometimes box/point tracks refer to frames outside the video.\n",
        "    max_frame = metadata.original_video_frames - 1\n",
        "    for track in box_tracks + point_tracks:\n",
        "      max_frame = max(max_frame, track.frames[-1])\n",
        "    if max_frame \u003e= metadata.original_video_frames:\n",
        "      logging.info(\"Video had %d frames, but annotation referenced frame %d.\",\n",
        "                   metadata.original_video_frames, max_frame + 1)\n",
        "      metadata.original_video_frames = max_frame + 1\n",
        "    grounded_object_questions = GroundedObjectQuestion.parse(\n",
        "        example,\n",
        "        available_box_tracks=set([b.track_id for b in box_tracks]))\n",
        "    multiple_choice_questions = MultipleChoiceQuestion.parse(example)\n",
        "    video_actions = VideoAction.parse(example)\n",
        "    video_features = get_features(example, float, \"action/features\")\n",
        "    return cls(metadata=metadata,\n",
        "               video_frames=video_frames,\n",
        "               video_features=video_features,\n",
        "               audio_wav=audio_wav,\n",
        "               point_tracks=point_tracks,\n",
        "               box_tracks=box_tracks,\n",
        "               audio_boxes=audio_boxes,\n",
        "               grounded_object_questions=grounded_object_questions,\n",
        "               multiple_choice_questions=multiple_choice_questions,\n",
        "               video_actions=video_actions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O0SNHtT62Ksj"
      },
      "outputs": [],
      "source": [
        "#@title Data processing functions\n",
        "\n",
        "def load_audio(\n",
        "    example: tf.train.SequenceExample\n",
        ") -\u003e np.ndarray:\n",
        "  \"\"\"Returns the audio sample.\"\"\"\n",
        "  pad_left = 0\n",
        "  [audio_start_us] = get_features(example, int, \"WAVEFORM/feature/timestamp\")\n",
        "  audio_start_us = audio_start_us[0]\n",
        "  if audio_start_us \u003e 0:\n",
        "    sample_rate_feat = example.context.feature[\"WAVEFORM/feature/sample_rate\"]\n",
        "    [audio_sample_rate] = sample_rate_feat.float_list.value\n",
        "    pad_left = round(audio_sample_rate * audio_start_us * 1e-6)\n",
        "  [samples] = get_features(example, float, \"WAVEFORM/feature/floats\")\n",
        "  ret = np.empty(pad_left + len(samples), dtype=np.float32)\n",
        "  ret[:pad_left] = 0\n",
        "  ret[pad_left:] = samples\n",
        "  return ret\n",
        "\n",
        "\n",
        "def load_video(\n",
        "    example: tf.train.SequenceExample\n",
        ") -\u003e np.ndarray:\n",
        "  \"\"\"Returns the video from a given example.\"\"\"\n",
        "  frame_features = get_features(example, bytes, \"image/encoded\")\n",
        "  num_frames = len(frame_features)\n",
        "\n",
        "  for t, frame_index in enumerate(range(num_frames)):\n",
        "    with io.BytesIO(frame_features[frame_index][0]) as f:\n",
        "      input_frame = PIL.Image.open(f)\n",
        "      if t == 0:\n",
        "        out_height = input_frame.height\n",
        "        out_width = input_frame.width\n",
        "        output_frames = np.empty(\n",
        "            (num_frames, out_height, out_width, 3), dtype=np.uint8)\n",
        "      output_frames[t] = np.frombuffer(\n",
        "          input_frame.tobytes(), dtype=np.uint8\n",
        "      ).reshape((out_height, out_width, 3))\n",
        "  return output_frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dmEGQv8Q2od2"
      },
      "outputs": [],
      "source": [
        "#@title Drawing and display utilities\n",
        "\n",
        "def display_video(frames, fps=30):\n",
        "  # Create and display temporary video from numpy array frames\n",
        "  # format (num_frames, height, width, channels)\n",
        "  imageio.mimwrite('tmp_video_display.mp4', frames, fps=fps); \n",
        "  display(mvp.ipython_display('tmp_video_display.mp4'))\n",
        "\n",
        "\n",
        "def display_frame(frame):\n",
        "  # Display a frame, converting from RGB to BGR for cv2.\n",
        "  cv2_imshow(frame[:, :, ::-1])\n",
        "\n",
        "\n",
        "def get_colors(num_colors: int) -\u003e Tuple[int, int, int]:\n",
        "  # Generate random colormaps for visualizing different objects and points.\n",
        "  colors = []\n",
        "  for i in np.arange(0., 360., 360. / num_colors):\n",
        "    hue = i / 360.\n",
        "    lightness = (50 + np.random.rand() * 10) / 100.\n",
        "    saturation = (90 + np.random.rand() * 10) / 100.\n",
        "    color = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "    color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
        "    colors.append(color)\n",
        "  random.seed(0)\n",
        "  random.shuffle(colors)\n",
        "  return colors\n",
        "\n",
        "COLORS = get_colors(num_colors=100)\n",
        "\n",
        "\n",
        "def try_different_dataset_notice(current, missing, suggestion):\n",
        "  msg = f\"\\n*** The current example from the `{current}` dataset does not \" \\\n",
        "        f\"contain any {missing} data.\\n\\n\" \\\n",
        "        f\"*** Try loading the `{suggestion}` dataset to \" \\\n",
        "        f\"visualise {missing} data.\" \n",
        "  print(f\"\\x1b[31m{msg}\\x1b[0m\")\n",
        "\n",
        "\n",
        "def paint_box(video: List[np.ndarray],\n",
        "              track: BoxTrack,\n",
        "              color: Tuple[int, int, int] = (255, 0, 0)):\n",
        "  num_frames, height, width, _ = video.shape\n",
        "  for box, frame_idx, human in zip(\n",
        "      track.boxes, track.frames, track.human_annotated):\n",
        "    if human:\n",
        "      label = f\"{track.label}*\"\n",
        "    else:\n",
        "      label = track.label\n",
        "    name = f'{track.track_id} : {label}'\n",
        "    frame = np.array(video[frame_idx])\n",
        "    y1 = int(round(box[0] * height))\n",
        "    x1 = int(round(box[1] * width))\n",
        "    y2 = int(round(box[2] * height))\n",
        "    x2 = int(round(box[3] * width))\n",
        "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2), color=color, thickness=2)\n",
        "    frame = cv2.putText(frame, name, (x1, y1 + 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.75, color, 2)\n",
        "    video[frame_idx] = frame\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_boxes(video, tracks: List[BoxTrack]):\n",
        "  for i, track in enumerate(tracks):\n",
        "    video = paint_box(video, track, COLORS[i])\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_point(\n",
        "    video: List[np.ndarray],\n",
        "    track: PointTrack,\n",
        "    color: Tuple[int, int, int] = (255, 0, 0),\n",
        "):\n",
        "  num_frames, height, width, _ = video.shape\n",
        "  for p, frame_idx, human in zip(\n",
        "      track.points, track.frames, track.human_annotated):\n",
        "    frame = video[frame_idx]\n",
        "    x = int(round(p[1] * width))\n",
        "    y = int(round(p[0] * height))\n",
        "    frame = cv2.circle(frame, (x, y), radius=10, color=color, thickness=-1)\n",
        "    video[frame_idx] = frame\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_points(video, tracks: List[PointTrack]):\n",
        "  for i, track in enumerate(tracks):\n",
        "    video = paint_point(video, track, COLORS[i])\n",
        "  return video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBf1ad5wOOcJ"
      },
      "source": [
        "#Parse and Visualise a SequenceExample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnbVEisyRX19"
      },
      "outputs": [],
      "source": [
        "#@title Parse an example to ParsedExample\n",
        "example = ParsedExample.parse(sequence_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WD4DgdV6au8u"
      },
      "outputs": [],
      "source": [
        "show_original_video = True  #@param {type: \"boolean\"}\n",
        "if show_original_video:\n",
        "  display_video(example.video_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwImmlCxvUvN"
      },
      "source": [
        "#Box Tracks\n",
        "Load and visualise the data associated with the annotated objects in a video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8Dg2xfu8cIx7"
      },
      "outputs": [],
      "source": [
        "#@markdown Draw annotated bounding boxes on video\n",
        "\n",
        "tmp_vid = example.video_frames.copy()\n",
        "show_all_tracks = True  #@param {type: \"boolean\"}\n",
        "show_track = 0  #@param {type: \"integer\"}\n",
        "if show_all_tracks:\n",
        "  _ = paint_boxes(tmp_vid, example.box_tracks)\n",
        "else:\n",
        "  _ = paint_box(tmp_vid, example.box_tracks[show_track], \n",
        "                COLORS[show_track])\n",
        "display_video(tmp_vid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O73eN2zevd5f"
      },
      "source": [
        "#Points\n",
        "Load and visualise an example from the `points` dataset containing points in the scene as they are tracked throughout the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QPQj0PLbE5xW"
      },
      "outputs": [],
      "source": [
        "#@markdown Show video with points overlayed.\n",
        "\n",
        "tmp_vid = example.video_frames.copy()\n",
        "show_all_tracks = True  #@param {type: \"boolean\"}\n",
        "show_track = 0  #@param {type: \"integer\"}\n",
        "\n",
        "num_point_tracks = len(example.point_tracks)\n",
        "if num_point_tracks:\n",
        "  if show_all_tracks:\n",
        "    _ = paint_points(tmp_vid, example.point_tracks)\n",
        "  else:\n",
        "    _ = paint_point(tmp_vid, example.point_tracks[show_track], \n",
        "                  COLORS[show_track])\n",
        "  display_video(tmp_vid)\n",
        "else:\n",
        "  try_different_dataset_notice(dataset, \"point tracking\", \"points_oss\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1_5YzyvmLX"
      },
      "source": [
        "#Actions\n",
        "Annotated actions that take place in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iiJUGjZCcmsh"
      },
      "outputs": [],
      "source": [
        "#@markdown List video actions\n",
        "human_readable_text = True # @param {type: \"boolean\"}\n",
        "\n",
        "action_labels = []\n",
        "action_start_times = []\n",
        "action_end_times = []\n",
        "print(f\"Track id\\tstart_frame\\tend_frame\\tstart_time\\t\\tend_time\\tLabel\")\n",
        "if example.video_actions:\n",
        "  if human_readable_text:\n",
        "    for va in example.video_actions:\n",
        "      action_labels.append(str(va.label))\n",
        "      action_start_times.append(va.start_time)\n",
        "      action_end_times.append(va.end_time)\n",
        "      print(f\"{va.track_id[0]}\\t\\t{va.start_frame[0]}\\t\\t{va.end_frame[0]}\"\n",
        "            f\"\\t\\t{va.start_time[0]:8}\\t\\t{va.end_time[0]:8}\\t{va.label[0]}\")\n",
        "else:\n",
        "  try_different_dataset_notice(dataset, \"video actions\", \"base_oss\")\n",
        "\n",
        "action_start_times = np.array(action_start_times).squeeze()\n",
        "action_end_times = np.array(action_end_times).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knHyyiwivqow"
      },
      "source": [
        "#Sounds\n",
        "List the sound events from an example in the `base` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W2jG4lnop33_"
      },
      "outputs": [],
      "source": [
        "#@title Audio\n",
        "#markdown Extract the audio for the video.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "from scipy.io.wavfile import write\n",
        "sample_rate = int(example.metadata.original_audio_sample_rate)\n",
        "\n",
        "wav = (example.audio_wav * 2**15).astype(int)\n",
        "write('test.wav', sample_rate, wav)\n",
        "Audio(wav, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ClA5Yqe3FISq"
      },
      "outputs": [],
      "source": [
        "#@title Sounds events\n",
        "#@markdown Print the sound events for a sample video.\n",
        "\n",
        "audio_labels = []\n",
        "audio_start_times = []\n",
        "audio_end_times = []\n",
        "print(f\"start\\tend\\tlabel\")\n",
        "if example.audio_boxes:\n",
        "  for audio in example.audio_boxes:\n",
        "    audio_labels.append(str(audio.audio_label))\n",
        "    audio_start_times.append(audio.start_time)\n",
        "    audio_end_times.append(audio.end_time)\n",
        "    print(f\"{audio.start_time:02.4f}\\t{audio.end_time:02.4f}\"\n",
        "          f\"\\t{audio.audio_label}\")\n",
        "\n",
        "audio_start_times = np.array(audio_start_times)\n",
        "audio_end_times = np.array(audio_end_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unv-gQIGzeR2"
      },
      "source": [
        "#Visualise timeline of events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RGa23JKjiD9J"
      },
      "outputs": [],
      "source": [
        "#@markdown Plot a timeline with the audio, sound events, action events and some frames.\n",
        "plt.figure(figsize=(14, 15))\n",
        "\n",
        "# Plot WAV\n",
        "plt.subplot(4,1,1)\n",
        "plt.title(\"Audio\")\n",
        "librosa.display.waveplot(example.audio_wav, sr=sample_rate)\n",
        "\n",
        "# Strip of frames\n",
        "plt.subplot(4,1,2)\n",
        "plt.title(\"Video Frames\")\n",
        "fstart = int(example.audio_boxes[3].start_time\n",
        "             * example.metadata.original_video_frame_rate)\n",
        "f_size = example.video_frames[0].shape\n",
        "small = tuple(reversed((np.array(f_size[:2]) / 4).astype(int)))\n",
        "strip = None\n",
        "num_frames = example.metadata.original_video_frames\n",
        "for i in range(0, num_frames, int(num_frames/4)):\n",
        "  frame = cv2.resize(example.video_frames[i], small)\n",
        "  if strip is None:\n",
        "    strip = np.array(frame)\n",
        "  else:\n",
        "    strip = np.concatenate([strip, frame], axis=1)\n",
        "plt.imshow(strip)\n",
        "\n",
        "# Plot audio events\n",
        "plt.subplot(4,1,3)\n",
        "plt.title(\"Audio Events\")\n",
        "plt.barh(range(len(audio_start_times)),\n",
        "         audio_end_times-audio_start_times,\n",
        "         left=audio_start_times)\n",
        "plt.yticks(range(len(audio_start_times)), audio_labels)\n",
        "\n",
        "# Plot video events\n",
        "plt.subplot(4,1,4)\n",
        "plt.title(\"Action Events\")\n",
        "plt.barh(range(len(action_start_times)),\n",
        "         action_end_times-action_start_times,\n",
        "         left=action_start_times)\n",
        "plt.yticks(range(len(action_start_times)), action_labels)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJiw40Mjvg-A"
      },
      "source": [
        "#Multiple choice Visual Questions and Answers\n",
        "Dataset containing general questions about activities in video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hY7kmm8GFAZ0"
      },
      "outputs": [],
      "source": [
        "#@markdown Load and print an example from the `multiple choice questions` dataset. \n",
        "human_readable = True # @param {type: \"boolean\"}\n",
        "if example.multiple_choice_questions:\n",
        "  for mcq in example.multiple_choice_questions:\n",
        "    if human_readable:\n",
        "      print(mcq.text)\n",
        "      for i, o in enumerate(mcq.options):\n",
        "        if i == mcq.answer_id:\n",
        "          answer = \" \u003c---- ANSWER\"\n",
        "        else:\n",
        "          answer = \"\"\n",
        "        print(f\"  {o}{answer}\")\n",
        "      print(\"\")\n",
        "    else:\n",
        "      print(mcq)\n",
        "else:\n",
        "  try_different_dataset_notice(dataset,\n",
        "                               \"multiple choice visual question and answer\",\n",
        "                               \"base_oss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cejk5wF2yoke"
      },
      "source": [
        "#Grounded Visual Questions and Answers\n",
        "Dataset containing questions about tracked objects in video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5PKnm8HOJXrG"
      },
      "outputs": [],
      "source": [
        "#@markdown Load and print an example from the `grounded questions` dataset.\n",
        "if example.grounded_object_questions:\n",
        "  human_readable = True # @param {type: \"boolean\"}\n",
        "  for goq in example.grounded_object_questions:\n",
        "    if human_readable:\n",
        "      print(goq.text)\n",
        "      print(\"Box Track IDs to follow:\")\n",
        "      for i, o in enumerate(goq.box_track_ids):\n",
        "        print(f\"  id {o}\")\n",
        "      print(\"\")\n",
        "    else:\n",
        "      print(goq)\n",
        "else:\n",
        "  try_different_dataset_notice(dataset, \"grounded visual questions and answers\",\n",
        "                               \"grounded_questions_oss\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "13IOkVPC0CX8Zr3Fvy5xflHYswUmQ8Wt4",
          "timestamp": 1665531786109
        },
        {
          "file_id": "13yMa_lxm_N6Vp4YmH_gnJeA9_3qsIs6L",
          "timestamp": 1665401700395
        },
        {
          "file_id": "1lxeuDid4JIbQBTDaJT1_ugaPfkbsUqKB",
          "timestamp": 1659002245306
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
